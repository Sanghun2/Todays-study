{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LciXgUynvCnv"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DISEWGyvtPB"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XSbry5_1vxM8"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = '''\n",
    "I'm only one call away.\n",
    "I'll be there to save the day.\n",
    "Superman got nothing on me, I'm only one call away.\n",
    "And when you're weak, I'll be strong, I'm gonna keep holding on.\n",
    "Now don't you worry it won't be long darling.\n",
    "And when you feel like hope is gone just run into my arms.\n",
    "'''\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hv3d0XFweLc"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qe7aS2ghKn6A"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "#하이픈, '는 붙여서 분절화해준다.\n",
    "print(TreebankWordTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPMLR9cSxKM0"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#단어들마다 나뉘되록 정규표현식을 작성해주세요.\n",
    "print(RegexpTokenizer('\\w+').tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzMN0htAdOaZ"
   },
   "outputs": [],
   "source": [
    "#지금까지는 단어 토크나이저였습니다.\n",
    "#토큰화 -> 어떤 기준대로 나눈다\n",
    "# 단어, 문장, 형태소\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEHfHZqqygm_"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#어간 추출기\n",
    "words = WordPunctTokenizer().tokenize(text)\n",
    "for word in words:\n",
    "    print(PorterStemmer().stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCUW5-I6y13p"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptfI7O_EztVj"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#words 리스트 안에서 sw 에 해당하는 단어를 제외시킨 리스트를 생성해주세요.\n",
    "#stopwords(불용어, 문장의 의미를 형성하는 것에 있어 큰 영향을 미치지 않는 단어들)\n",
    "sw = ['.', ',', \"'\"]\n",
    "\n",
    "sw_removed = []\n",
    "for i in words:\n",
    "    if i.lower() not in sw:\n",
    "        sw_removed.append(i)\n",
    "\n",
    "print(sw_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8V6mDeU2JZl"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_list = Counter(sw_removed)\n",
    "print(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIqA7fK72qRs"
   },
   "outputs": [],
   "source": [
    "common_cl = count_list.most_common(10)\n",
    "print(common_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SyucbxrP285Y"
   },
   "outputs": [],
   "source": [
    "common_cl_dict = {}\n",
    "\n",
    "#빈도순위대로 0위~9위 (빈도수가 가장 높은게 0등)\n",
    "#딕셔너리 형태로, '단어':빈도순위\n",
    "i = 0\n",
    "for (key, value) in common_cl:\n",
    "    common_cl_dict[key] = i\n",
    "    i = i+1\n",
    "\n",
    "print(common_cl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4DV2iRO4z0n"
   },
   "outputs": [],
   "source": [
    "oh_vector_list = []\n",
    "\n",
    "for value in common_cl_dict.values():\n",
    "    oh_vector = [0] * len(common_cl_dict)\n",
    "    oh_vector[value] = 1\n",
    "    oh_vector_list.append(oh_vector)\n",
    "\n",
    "print(oh_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEcm1BM17f9C"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "my_wc = WordCloud(background_color='white')\n",
    "plt.imshow(my_wc.generate_from_frequencies(count_list))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tokenizer.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
